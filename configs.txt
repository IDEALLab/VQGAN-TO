# Configuration file for VQGAN ablation studies (stages 1 and 2)
# Format: experiment_name:param1=value1,param2=value2,param3=value3
# ALL NAMES MUST BE DIFFERENT


#################### VQGAN ####################
# Baseline with default parameters
baseline:
repeat_baseline:
repeat2_baseline:

# 1) Latent dimension ablation
latent_32:latent_dim=32
latent_16:latent_dim=16
latent_8:latent_dim=8
latent_4:latent_dim=4
latent_4_ND:latent_dim=4,disc_start=999999
repeat_latent_4_ND:latent_dim=4,disc_start=999999
repeat2_latent_4_ND:latent_dim=4,disc_start=999999

# 2) Codebook vector count ablation
codebook_128:num_codebook_vectors=128
codebook_64:num_codebook_vectors=64
codebook_32:num_codebook_vectors=32
repeat_codebook_32:num_codebook_vectors=32
repeat2_codebook_32:num_codebook_vectors=32

# 3) Learning rate ablation
lr_med:learning_rate=2e-04
lr_high:learning_rate=2e-03
lr_med_ND:learning_rate=2e-04,disc_start=999999
repeat_lr_med_ND:learning_rate=2e-04,disc_start=999999
repeat2_lr_med_ND:learning_rate=2e-04,disc_start=999999
lr_high_ND:learning_rate=2e-03,disc_start=999999

# 4) Discriminator start ablation
disc_early:disc_start=5
disc_none:disc_start=999999
repeat_disc_none:disc_start=999999
repeat2_disc_none:disc_start=999999

# 5) Spectral normalization of the discriminator
spec_norm_disc:spectral_disc=true

# 6) Network size ablations (medium, hybrid, and small, respectively)
net_med:decoder_channels=256 256 256 128 128,encoder_channels=128 128 128 256 256 256
net_hybrid:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256
net_small:decoder_channels=128 128 128 64 64,encoder_channels=64 64 64 128 128 128

# 7) Residual blocks ablation
res_more:decoder_num_res_blocks=4,encoder_num_res_blocks=3
res_less:decoder_num_res_blocks=2,encoder_num_res_blocks=1

# 8) Attention resolution ablation
attn_med:decoder_attn_resolutions=16 32,encoder_attn_resolutions=16 32
attn_high:decoder_attn_resolutions=16 32 64,encoder_attn_resolutions=16 32 64

# Combinations after initial testing:
c1:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=8,num_codebook_vectors=64,learning_rate=2.25e-05,disc_start=999999
c2:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=8,num_codebook_vectors=64,learning_rate=2e-04,disc_start=999999
c3:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=8,num_codebook_vectors=32,learning_rate=2.25e-05,disc_start=999999
c4:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=8,num_codebook_vectors=32,learning_rate=2e-04,disc_start=999999

c5:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=4,num_codebook_vectors=64,learning_rate=2.25e-05,disc_start=999999
c6:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=4,num_codebook_vectors=64,learning_rate=2e-04,disc_start=999999
repeat2_c6:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=4,num_codebook_vectors=64,learning_rate=2e-04,disc_start=999999
c7:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=4,num_codebook_vectors=32,learning_rate=2.25e-05,disc_start=999999
c8:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=4,num_codebook_vectors=32,learning_rate=2e-04,disc_start=999999

# Also try latent_dim 4 or even 2? Start without discriminator
# After this: pick the best above and test with different discriminator start times (0-5 epochs in). When using discriminator, likely stick to learning_rate=2.25e-05

# From c6 (best): lower latent_dim to 2, 1, and double codebook vectors to 128, 256
c9:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=2,num_codebook_vectors=128,learning_rate=2e-04,disc_start=999999
c10:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=1,num_codebook_vectors=256,learning_rate=2e-04,disc_start=999999

c11:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=4,num_codebook_vectors=256,learning_rate=2e-04,disc_start=999999
c12:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=4,num_codebook_vectors=512,learning_rate=2e-04,disc_start=999999
c13:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=4,num_codebook_vectors=1024,learning_rate=2e-04,disc_start=999999
c14:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=4,num_codebook_vectors=1024,learning_rate=2e-03,disc_start=999999

#################### CVQGAN ####################
cvq:is_c=true,image_channels=3,learning_rate=2e-04,disc_start=9999999,epochs=1000,sample_interval=10
# Hint: image_channels is the number of conditions in this case


################## TRANSFORMER #################
# Ablation study with baseline and best-performing Stage 1 model: Decreasing transformer size (starting w/ dropout=0.0)
Tr_baseline:is_t=true,model_name=baseline,c_model_name=cvq,epochs=1000,sample_interval=10
Tr_baseline_1:is_t=true,model_name=baseline,c_model_name=cvq,epochs=1000,sample_interval=10,n_layer=12,n_head=12,n_embd=384
Tr_baseline_2:is_t=true,model_name=baseline,c_model_name=cvq,epochs=1000,sample_interval=10,n_layer=8,n_head=8,n_embd=384
Tr_baseline_3:is_t=true,model_name=baseline,c_model_name=cvq,epochs=1000,sample_interval=10,n_layer=8,n_head=8,n_embd=192
Tr_baseline_4:is_t=true,model_name=baseline,c_model_name=cvq,epochs=1000,sample_interval=10,n_layer=6,n_head=6,n_embd=96
Tr_baseline_5:is_t=true,model_name=baseline,c_model_name=cvq,epochs=1000,sample_interval=10,n_layer=4,n_head=4,n_embd=48

Tr_c6:is_t=true,model_name=c6,c_model_name=cvq,epochs=1000,sample_interval=10
Tr_c6_1:is_t=true,model_name=c6,c_model_name=cvq,epochs=1000,sample_interval=10,n_layer=12,n_head=12,n_embd=384
Tr_c6_2:is_t=true,model_name=c6,c_model_name=cvq,epochs=1000,sample_interval=10,n_layer=8,n_head=8,n_embd=384
Tr_c6_3:is_t=true,model_name=c6,c_model_name=cvq,epochs=1000,sample_interval=10,n_layer=8,n_head=8,n_embd=192
Tr_c6_4:is_t=true,model_name=c6,c_model_name=cvq,epochs=1000,sample_interval=10,n_layer=6,n_head=6,n_embd=96
Tr_c6_5:is_t=true,model_name=c6,c_model_name=cvq,epochs=1000,sample_interval=10,n_layer=4,n_head=4,n_embd=48

Tr_c11:is_t=true,model_name=c11,c_model_name=cvq,epochs=1000,sample_interval=10
Tr_c11_repeat:is_t=true,model_name=c11,c_model_name=cvq,epochs=1000,sample_interval=10
Tr_c11_repeat_2:is_t=true,model_name=c11,c_model_name=cvq,epochs=1000,sample_interval=10
Tr_c11_repeat_3:is_t=true,model_name=c11,c_model_name=cvq,epochs=1000,sample_interval=10
Tr_c11_1:is_t=true,model_name=c11,c_model_name=cvq,epochs=1000,sample_interval=10,n_layer=12,n_head=12,n_embd=384
Tr_c11_2:is_t=true,model_name=c11,c_model_name=cvq,epochs=1000,sample_interval=10,n_layer=8,n_head=8,n_embd=384
Tr_c11_3:is_t=true,model_name=c11,c_model_name=cvq,epochs=1000,sample_interval=10,n_layer=8,n_head=8,n_embd=192
Tr_c11_4:is_t=true,model_name=c11,c_model_name=cvq,epochs=1000,sample_interval=10,n_layer=6,n_head=6,n_embd=96
Tr_c11_5:is_t=true,model_name=c11,c_model_name=cvq,epochs=1000,sample_interval=10,n_layer=4,n_head=4,n_embd=48

Tr_c11_dropout:is_t=true,model_name=c11,c_model_name=cvq,epochs=1000,sample_interval=10,dropout=0.3
Tr_c11_3_dropout:is_t=true,model_name=c11,c_model_name=cvq,epochs=1000,sample_interval=10,n_layer=8,n_head=8,n_embd=192,dropout=0.3
Tr_c11_dropout_pkeep:is_t=true,model_name=c11,c_model_name=cvq,epochs=1000,sample_interval=10,dropout=0.3,pkeep=0.9

Tr_c6_dropout_pkeep:is_t=true,model_name=c6,c_model_name=cvq,epochs=1000,sample_interval=10,dropout=0.3,pkeep=0.5
Tr_c6_dropout_pkeep_2:is_t=true,model_name=c6,c_model_name=cvq,epochs=1000,sample_interval=10,dropout=0.1,pkeep=0.9
Tr_c6_dropout_pkeep_3:is_t=true,model_name=c6,c_model_name=cvq,epochs=1000,sample_interval=10,dropout=0.3,pkeep=0.9
Tr_c6_dropout_pkeep_3_repeat:is_t=true,model_name=c6,c_model_name=cvq,epochs=1000,sample_interval=10,dropout=0.3,pkeep=0.9
Tr_c6_dropout_pkeep_3_repeat2:is_t=true,model_name=c6,c_model_name=cvq,epochs=1000,sample_interval=10,dropout=0.3,pkeep=0.9
Tr_c6_dropout_pkeep_4:is_t=true,model_name=c6,c_model_name=cvq,epochs=1000,sample_interval=10,dropout=0.5,pkeep=0.9
Tr_c6_dropout_pkeep_5:is_t=true,model_name=c6,c_model_name=cvq,epochs=1000,sample_interval=10,dropout=0.3,pkeep=0.9,t_learning_rate=6e-4
Tr_c6_dropout_pkeep_6:is_t=true,model_name=c6,c_model_name=cvq,epochs=1000,sample_interval=10,dropout=0.3,pkeep=0.9,n_layer=24,n_head=16,n_embd=1024
Tr_c6_dropout_pkeep_7:is_t=true,model_name=c6,c_model_name=cvq,epochs=1000,sample_interval=10,dropout=0.3,pkeep=0.9,n_layer=36,n_head=20,n_embd=1280

###########################################################################
###########################################################################
###########################################################################
###########################################################################
###########################################################################
nv_c11:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=4,num_codebook_vectors=256,learning_rate=2e-04,disc_start=999999
nv_c6:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=4,num_codebook_vectors=64,learning_rate=2e-04,disc_start=999999
nv_baseline:
nv_latent_4:latent_dim=4
nv_latent_4_ND:latent_dim=4,disc_start=999999
nv_lr_med:learning_rate=2e-04
nv_lr_med_ND:learning_rate=2e-04,disc_start=999999
nv_net_hybrid:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256
nv_disc_none:disc_start=999999
nv_codebook_256:num_codebook_vectors=256
nv_codebook_128:num_codebook_vectors=128
nv_codebook_64:num_codebook_vectors=64
nv_codebook_32:num_codebook_vectors=32

nv_cvq:is_c=true,image_channels=3,learning_rate=2e-04,disc_start=9999999,epochs=1000,sample_interval=10
nv_cvq_large:is_c=true,image_channels=3,learning_rate=2e-04,disc_start=9999999,epochs=1000,sample_interval=10,c_fmap_dim=16

nv_Tr_c11:is_t=true,model_name=nv_c11,c_model_name=nv_cvq,epochs=1000,sample_interval=10,dropout=0.3,pkeep=0.9
nv_Tr_c6:is_t=true,model_name=nv_c6,c_model_name=nv_cvq,epochs=1000,sample_interval=10,dropout=0.3,pkeep=0.9
nv_Tr_c11_L:is_t=true,model_name=nv_c11,c_model_name=nv_cvq_large,epochs=1000,sample_interval=10,dropout=0.3,pkeep=0.9
nv_Tr_c6_L:is_t=true,model_name=nv_c6,c_model_name=nv_cvq_large,epochs=1000,sample_interval=10,dropout=0.3,pkeep=0.9