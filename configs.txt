# Configuration file for VQGAN ablation studies (stages 1 and 2)
# Format: experiment_name:param1=value1,param2=value2,param3=value3
# ALL NAMES MUST BE DIFFERENT


#################### VQGAN ####################
# Baseline with default parameters
baseline:
repeat_baseline:

# 1) Latent dimension ablation
latent_32:latent_dim=32
latent_16:latent_dim=16
latent_8:latent_dim=8
latent_4:latent_dim=4
latent_4_ND:latent_dim=4,disc_start=999999
repeat_latent_4_ND:latent_dim=4,disc_start=999999

# 2) Codebook vector count ablation
codebook_128:num_codebook_vectors=128
codebook_64:num_codebook_vectors=64
codebook_32:num_codebook_vectors=32
repeat_codebook_32:num_codebook_vectors=32

# 3) Learning rate ablation
lr_med:learning_rate=2e-04
lr_high:learning_rate=2e-03
lr_med_ND:learning_rate=2e-04,disc_start=999999
repeat_lr_med_ND:learning_rate=2e-04,disc_start=999999
lr_high_ND:learning_rate=2e-03,disc_start=999999

# 4) Discriminator start ablation
disc_early:disc_start=5
disc_none:disc_start=999999
repeat_disc_none:disc_start=999999

# 5) Spectral normalization of the discriminator
spec_norm_disc:spectral_disc=true

# 6) Network size ablations (medium, hybrid, and small, respectively)
net_med:decoder_channels=256 256 256 128 128,encoder_channels=128 128 128 256 256 256
net_hybrid:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256
net_small:decoder_channels=128 128 128 64 64,encoder_channels=64 64 64 128 128 128

# 7) Residual blocks ablation
res_more:decoder_num_res_blocks=4,encoder_num_res_blocks=3
res_less:decoder_num_res_blocks=2,encoder_num_res_blocks=1

# 8) Attention resolution ablation
attn_med:decoder_attn_resolutions=16 32,encoder_attn_resolutions=16 32
attn_high:decoder_attn_resolutions=16 32 64,encoder_attn_resolutions=16 32 64

# Combinations after initial testing:
c1:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=8,num_codebook_vectors=64,learning_rate=2.25e-05,disc_start=999999
c2:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=8,num_codebook_vectors=64,learning_rate=2e-04,disc_start=999999
c3:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=8,num_codebook_vectors=32,learning_rate=2.25e-05,disc_start=999999
c4:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=8,num_codebook_vectors=32,learning_rate=2e-04,disc_start=999999

c5:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=4,num_codebook_vectors=64,learning_rate=2.25e-05,disc_start=999999
c6:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=4,num_codebook_vectors=64,learning_rate=2e-04,disc_start=999999
c7:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=4,num_codebook_vectors=32,learning_rate=2.25e-05,disc_start=999999
c8:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=4,num_codebook_vectors=32,learning_rate=2e-04,disc_start=999999

# Also try latent_dim 4 or even 2? Start without discriminator
# After this: pick the best above and test with different discriminator start times (0-5 epochs in). When using discriminator, likely stick to learning_rate=2.25e-05

# From c6 (best): lower latent_dim to 2, 1, and double codebook vectors to 128, 256
c9:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=2,num_codebook_vectors=128,learning_rate=2e-04,disc_start=999999
c10:decoder_channels=256 128 128 64 64,encoder_channels=64 64 64 128 128 256,latent_dim=1,num_codebook_vectors=256,learning_rate=2e-04,disc_start=999999


#################### CVQGAN ####################
cvq:is_c=true,image_channels=3,learning_rate=2e-04,disc_start=9999999,epochs=1000,sample_interval=10
# Hint: image_channels is the number of conditions in this case


################## TRANSFORMER #################
Tr_baseline:is_t=true,model_name=baseline,c_model_name=cvq
Tr_baseline_long:is_t=true,model_name=baseline,c_model_name=cvq,dropout=0.1,epochs=500
Tr_c6:is_t=true,model_name=c6,c_model_name=cvq
Tr_c6_mini:is_t=true,model_name=c6,c_model_name=cvq,n_layer=4,n_head=4,n_embd=128,dropout=0.1
Tr_c6_mini_long:is_t=true,model_name=c6,c_model_name=cvq,n_layer=4,n_head=4,n_embd=128,dropout=0.1,epochs=500
Tr_c6_mini_1000:is_t=true,model_name=c6,c_model_name=cvq,n_layer=4,n_head=4,n_embd=128,dropout=0.1,epochs=1000,sample_interval=10
Tr_baseline_mini_1000:is_t=true,model_name=baseline,c_model_name=cvq,n_layer=4,n_head=4,n_embd=128,dropout=0.1,epochs=1000,sample_interval=10
Tr_c6_mini_1000_highLR:is_t=true,model_name=c6,c_model_name=cvq,n_layer=4,n_head=4,n_embd=128,dropout=0.1,epochs=1000,sample_interval=10,t_learning_rate=4.5e-05

# Noted that in the initial transformer config setup, changing learning_rate had no effect on training
    # lr was manually set to 4.5e-06 in configure_optimizers() of training_transformer.py
    # Addressed by creating a new argument t_learning_rate with default 4.5e-06